{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建基本模块--神经元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:46:03.060298Z",
     "start_time": "2019-03-27T11:46:03.028058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9990889488055994\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # our activation function: f(x) = 1 / (1 * e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class Neuron():\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        # weight inputs, add bias, then use the activation function\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "    \n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "# inputs\n",
    "x = np.array([2, 3])   # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x)) # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 搭建神经网络\n",
    "- 这个网络有2个输入、一个包含2个神经元的隐藏层(h1和h2)，包含1个神经元的输出层o1\n",
    "- 我们假设上面的网络里所有的神经元都具有相同的权重w=\\[0,1\\]，和偏置b=0，激活函数都是sigmoid，那么输出结果是什么呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7216325609518421\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetworks():\n",
    "    \"\"\"\n",
    "    A neural network with:\n",
    "      - 2 inputs\n",
    "      - a hidden layer with 2 neurons (h1, h2)\n",
    "      - an output layer with 1 neuron (o1)\n",
    "    Each neural has the same weights and bias:\n",
    "      - w = [0, 1]\n",
    "      - b = 0\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "        \n",
    "        # The Neuron class here is from the previous section\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "        # The inputs for o1 are the outputs from h1 and h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        return out_o1\n",
    "        \n",
    "network = OurNeuralNetworks()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练神经网络\n",
    "- 在训练神经网络之前，需要一个标准定义它到底好不好，以便我们进行改进，这就是**损失**。\n",
    "- 比如使用均方误差（MSE）来定义损失，即所有数据方差的平均值。预测结果越好，损失就越低。\n",
    "- 训练神经网络就是将损失最小化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算损失函数的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 减少神经网络损失\n",
    "- 我们知道，改变网络的权重和偏置可以影响预测值，但是应该怎么做呢？\n",
    "- 预测值是由一系列网络权重和偏置计算出来的，所以损失函数实际上是包含多个权重、偏置的多元函数：L(w1, w2, w3, w4, w5, w6, b1, b2, b3)。\n",
    "- 如果调整一下w1，损失函数是会变大还是变小？我们需要知道**L**对**w1**的偏导数是正是负才能回答这个问题。\n",
    "- 根据链式求导公式计算**L**对**w1**的偏导数，这种向后计算偏导数的系统称为反向传播（backpropagation）。\n",
    "- 带入实际数据计算得到的结果告诉我们：如果增大**w1**，损失函数**L**会有一个非常小的增长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机梯度下降（SGD）\n",
    "- 经过前面的运算，我们已经有了训练神经网络所有数据。接下来该如何操作呢？\n",
    "- SGD定义了改变权重和偏置的方法${w_1 \\leftarrow w_1 - \\eta\\frac{\\partial L}{\\partial w_1}}$，${\\eta}$是一个常数，称为学习率。\n",
    "- 如果我们用这种方法去逐步改变网络的权重**w**和偏置**b**，损失函数会缓慢地降低，从而改进我们地神经网络。\n",
    "- 训练流程如下：\n",
    "    - 从数据集中选择一个样本；\n",
    "    - 计算损失函数对所有权重地偏置地偏导数；\n",
    "    - 使用更新公式更新每个权重和偏置；\n",
    "    - 回到第1步。\n",
    "- 用代码实现这个过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch %d loss: %.3f (0, 0.4588985952447696)\n",
      "Epoch %d loss: %.3f (10, 0.4559672713951668)\n",
      "Epoch %d loss: %.3f (20, 0.45225347378751546)\n",
      "Epoch %d loss: %.3f (30, 0.44768477381050636)\n",
      "Epoch %d loss: %.3f (40, 0.44221828512636424)\n",
      "Epoch %d loss: %.3f (50, 0.4358435174082699)\n",
      "Epoch %d loss: %.3f (60, 0.42857310108322844)\n",
      "Epoch %d loss: %.3f (70, 0.42041999804519775)\n",
      "Epoch %d loss: %.3f (80, 0.41136714656007467)\n",
      "Epoch %d loss: %.3f (90, 0.40134328327038493)\n",
      "Epoch %d loss: %.3f (100, 0.3902227392324411)\n",
      "Epoch %d loss: %.3f (110, 0.37786443547696913)\n",
      "Epoch %d loss: %.3f (120, 0.36419404986784165)\n",
      "Epoch %d loss: %.3f (130, 0.34931043401364625)\n",
      "Epoch %d loss: %.3f (140, 0.3335656502267289)\n",
      "Epoch %d loss: %.3f (150, 0.3175524598795173)\n",
      "Epoch %d loss: %.3f (160, 0.3019733678486572)\n",
      "Epoch %d loss: %.3f (170, 0.28745273933991766)\n",
      "Epoch %d loss: %.3f (180, 0.274400173670289)\n",
      "Epoch %d loss: %.3f (190, 0.2629823600871015)\n",
      "Epoch %d loss: %.3f (200, 0.2531770649502129)\n",
      "Epoch %d loss: %.3f (210, 0.2448513815292102)\n",
      "Epoch %d loss: %.3f (220, 0.2378261868608531)\n",
      "Epoch %d loss: %.3f (230, 0.2319166548004255)\n",
      "Epoch %d loss: %.3f (240, 0.2269530425667047)\n",
      "Epoch %d loss: %.3f (250, 0.22278892582644957)\n",
      "Epoch %d loss: %.3f (260, 0.21930256191005887)\n",
      "Epoch %d loss: %.3f (270, 0.21639496079478962)\n",
      "Epoch %d loss: %.3f (280, 0.21398667684446257)\n",
      "Epoch %d loss: %.3f (290, 0.212014362753892)\n",
      "Epoch %d loss: %.3f (300, 0.21042757364563405)\n",
      "Epoch %d loss: %.3f (310, 0.2091860111076636)\n",
      "Epoch %d loss: %.3f (320, 0.20825724546376495)\n",
      "Epoch %d loss: %.3f (330, 0.20761488405388998)\n",
      "Epoch %d loss: %.3f (340, 0.20723712578756712)\n",
      "Epoch %d loss: %.3f (350, 0.20710563639020194)\n",
      "Epoch %d loss: %.3f (360, 0.20720468295400796)\n",
      "Epoch %d loss: %.3f (370, 0.2075204745079437)\n",
      "Epoch %d loss: %.3f (380, 0.20804066427051016)\n",
      "Epoch %d loss: %.3f (390, 0.2087539776360392)\n",
      "Epoch %d loss: %.3f (400, 0.2096499372232414)\n",
      "Epoch %d loss: %.3f (410, 0.21071866236731657)\n",
      "Epoch %d loss: %.3f (420, 0.21195072534278794)\n",
      "Epoch %d loss: %.3f (430, 0.21333705051738885)\n",
      "Epoch %d loss: %.3f (440, 0.21486884572731282)\n",
      "Epoch %d loss: %.3f (450, 0.21653755758883592)\n",
      "Epoch %d loss: %.3f (460, 0.21833484435692688)\n",
      "Epoch %d loss: %.3f (470, 0.2202525614203781)\n",
      "Epoch %d loss: %.3f (480, 0.2222827556756739)\n",
      "Epoch %d loss: %.3f (490, 0.22441766591981008)\n",
      "Epoch %d loss: %.3f (500, 0.22664972710166165)\n",
      "Epoch %d loss: %.3f (510, 0.22897157681592503)\n",
      "Epoch %d loss: %.3f (520, 0.23137606284702575)\n",
      "Epoch %d loss: %.3f (530, 0.2338562508990223)\n",
      "Epoch %d loss: %.3f (540, 0.23640543190186045)\n",
      "Epoch %d loss: %.3f (550, 0.23901712848029089)\n",
      "Epoch %d loss: %.3f (560, 0.2416851003217689)\n",
      "Epoch %d loss: %.3f (570, 0.24440334829348198)\n",
      "Epoch %d loss: %.3f (580, 0.24716611724396328)\n",
      "Epoch %d loss: %.3f (590, 0.24996789748763185)\n",
      "Epoch %d loss: %.3f (600, 0.25280342501586023)\n",
      "Epoch %d loss: %.3f (610, 0.25566768050965616)\n",
      "Epoch %d loss: %.3f (620, 0.2585558872497922)\n",
      "Epoch %d loss: %.3f (630, 0.26146350803270196)\n",
      "Epoch %d loss: %.3f (640, 0.26438624120660054)\n",
      "Epoch %d loss: %.3f (650, 0.26732001594367116)\n",
      "Epoch %d loss: %.3f (660, 0.2702609868620013)\n",
      "Epoch %d loss: %.3f (670, 0.2732055281062657)\n",
      "Epoch %d loss: %.3f (680, 0.2761502269896836)\n",
      "Epoch %d loss: %.3f (690, 0.27909187729216706)\n",
      "Epoch %d loss: %.3f (700, 0.28202747230127)\n",
      "Epoch %d loss: %.3f (710, 0.28495419767392965)\n",
      "Epoch %d loss: %.3f (720, 0.2878694241883254)\n",
      "Epoch %d loss: %.3f (730, 0.29077070044667336)\n",
      "Epoch %d loss: %.3f (740, 0.29365574558158025)\n",
      "Epoch %d loss: %.3f (750, 0.29652244201079736)\n",
      "Epoch %d loss: %.3f (760, 0.2993688282779153)\n",
      "Epoch %d loss: %.3f (770, 0.30219309200977346)\n",
      "Epoch %d loss: %.3f (780, 0.30499356301513453)\n",
      "Epoch %d loss: %.3f (790, 0.30776870654351707)\n",
      "Epoch %d loss: %.3f (800, 0.310517116717976)\n",
      "Epoch %d loss: %.3f (810, 0.313237510151062)\n",
      "Epoch %d loss: %.3f (820, 0.31592871974916076)\n",
      "Epoch %d loss: %.3f (830, 0.31858968870689397)\n",
      "Epoch %d loss: %.3f (840, 0.3212194646902212)\n",
      "Epoch %d loss: %.3f (850, 0.3238171942042989)\n",
      "Epoch %d loss: %.3f (860, 0.32638211714000176)\n",
      "Epoch %d loss: %.3f (870, 0.3289135614912533)\n",
      "Epoch %d loss: %.3f (880, 0.3314109382339294)\n",
      "Epoch %d loss: %.3f (890, 0.3338737363560527)\n",
      "Epoch %d loss: %.3f (900, 0.33630151802825975)\n",
      "Epoch %d loss: %.3f (910, 0.33869391390306386)\n",
      "Epoch %d loss: %.3f (920, 0.3410506185312276)\n",
      "Epoch %d loss: %.3f (930, 0.343371385883564)\n",
      "Epoch %d loss: %.3f (940, 0.34565602496668213)\n",
      "Epoch %d loss: %.3f (950, 0.3479043955215459)\n",
      "Epoch %d loss: %.3f (960, 0.35011640379419645)\n",
      "Epoch %d loss: %.3f (970, 0.3522919983685763)\n",
      "Epoch %d loss: %.3f (980, 0.3544311660520449)\n",
      "Epoch %d loss: %.3f (990, 0.35653392780487564)\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true and y_pred are numpy arrays of the same length\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "class OurNeuralNetwork():\n",
    "    \"\"\"\n",
    "    A neural network with:\n",
    "      - 2 inputs\n",
    "      - a hidden layer with 2 neurons (h1, h2)\n",
    "      - an output layer with 1 neuron (o1)\n",
    "      \n",
    "    *** DISCLAIMER ***\n",
    "    The code below is intend to be simple and educational, NOT optimal.\n",
    "    Real neural net code looks nothing like this. Do NOT use this code.\n",
    "    Instead, read/run it to understand how this specific network works.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # weights\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "        # biases\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        # x is a numpy array with 2 elements, for example [input1, input2]\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "    \n",
    "    def train(self, data, all_y_trues):\n",
    "        \"\"\"\n",
    "        - data is a (n x 2) numpy array, n = # samples in the dataset.\n",
    "        - all_y_trues is a numpy array with n elements.\n",
    "        Elements in all_y_trues correspond to those in data.\n",
    "        \"\"\"\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # number of times to loop through the entire dataset\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # Do a feedforward (we'll need these values later)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "                \n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "                \n",
    "                sum_o1 = self.w5 * x[0] + self.w6 * x[1] + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "                \n",
    "                # - - - Calculate partial derivatives.\n",
    "                # - - - Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "                \n",
    "                # Neuron o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "                \n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "                \n",
    "                # Neuron h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "                \n",
    "                # Neuron h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "                \n",
    "                # - - - update weights and biases\n",
    "                # Neuron h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "                \n",
    "                # Neuron h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "                \n",
    "                # Neuron o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "                \n",
    "            # - - - Calculate total loss at the end of each epoch\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                print(\"Epoch %d loss: %.3f\", (epoch, loss))\n",
    "                \n",
    "# Define dataset\n",
    "data = np.array([\n",
    "    [-2, -1], # Alice\n",
    "    [25, 6],  # Bob\n",
    "    [17, 4],  # Charlie\n",
    "    [-15, -6] # diana\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "    1, # Alice\n",
    "    0, # Bob\n",
    "    0, # Charlie\n",
    "    1 # diana\n",
    "])\n",
    "\n",
    "# Train our neural network!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
